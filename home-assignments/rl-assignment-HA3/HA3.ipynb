{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20a4d65ee77906a86bc39bc4046a2a36",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use a cloud GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_fname = \"XXX.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"\" \n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736e393ef62f60d5e70432726e7209e0",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','9'), \"You are not running Python 3.9. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d199303c73ec86d25177caf39e385f",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nb_dirname = os.path.abspath('')\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in ['IHA1', 'IHA2', 'HA1', 'HA2', 'HA3'], \\\n",
    "    '[ERROR] The notebook appears to have been moved from its original directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78c7227b049bb147e6c363affb6dae8",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    display(HTML(r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(nb_fname=nb_fname)))\n",
    "except NameError:\n",
    "    assert False, 'Make sure to fill in the nb_fname variable above!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb700528d4644601c1a8c91ef1d84635",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "850c0f2c3ed462de9f2ee9df2bd820a7",
     "grade": false,
     "grade_id": "cell-2f332c3ca731afc6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Home Assignment 3\n",
    "This home assignment is about reinforcement learning and deep reinforcement learning. In Section 1, we introduce a small grid world that we will use in some of the examples. Section 2, then briefly covers model-based reinforcement learning techniques (policy evaluation, policy iteration and value iteration), whereas Section 3 touches upon model-free reinforcement learning techniques (Q-learning). Finally, we turn our attention to model-free deep reinforcement learning (DQN), i.e., neural networks as function approximators, in Section 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3a9f1e82b0aaa91e0a17946b14ccc26",
     "grade": false,
     "grade_id": "cell-8122dcb8d8ca1c9e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Section 1: Grid-World Introduction\n",
    "\n",
    "We first describe our standard grid world, which we will use in Sections 1, 2 and 3. As illustrated in the figure below, we have a 5x3 grid world with 15 cells. Each cell is color-coded to represent the characteristics of the cell. There are 11 white and two black cells along with one red and one green cell. We also have an indigo colored Terminal State (T.S.) cell outside our 5x3 grid. All cells represent reachable states except for the two black cells that represent walls that the agent cannot enter. For simplicity, we write, e.g., \"the red state\" when referring to the state that corresponds to a red cell.\n",
    "\n",
    "![Grid World](./grid_world.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75938e8f46b5dafbf6e8cf7133181daa",
     "grade": false,
     "grade_id": "cell-b4e5d5337fbaa0e5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Environment Dynamics\n",
    "\n",
    "The agent can choose between one of the four available actions: Up, Down, Left or Right. The set of all actions is called an action space and we can write our action space as $\\cal{A}$ $=\\{↑, →, ↓, ←\\}$. We will use the words $\\{\\text{Up}, \\text{Right}, \\text{Down}, \\text{Left}\\}$ and the symbols $\\{↑, →, ↓, ←\\}$ interchangeably.\n",
    "\n",
    "If the agent is in a white state and moves in the direction of a wall, either the boundary of the grid world or the black wall, the agent remains in its current state until the next time step. The red and green states are special states where any action taken by the agent takes it to the Terminal State (T.S.) in the next time step, and the episode ends.\n",
    "\n",
    "We will use both deterministic and stochastic transition/dynamics models for the white states. When the transition model is deterministic, taking an action $a$ (e.g., Up) takes the agent in the intended direction (e.g., Up) with probability 1. When the transition model is stochastic, taking an action $a$ takes the agent in the intended direction with probability $p$, and with probability $1-p$, the agent goes in a different direction depending on the specification of the stochastic transition model.\n",
    "\n",
    "### 1.2 Reward Function\n",
    "\n",
    "The immediate reward $R_{t+1}$ is 0 with probability 1 in the white states for any action. For the green state, the reward is always 1, and for the red state, always -1, regardless of the action.\n",
    "\n",
    "### 1.3 State Representation\n",
    "\n",
    "The notations used to define the states are illustrated in the figure below.\n",
    "\n",
    "![title](./state_rep.png)\n",
    "\n",
    "Each state has been given an integer number from 0 to 15, as illustrated in the figure. We note that the green state is 4, the red state is 9, the terminal state is 15 whereas 6 and 7 are unreachable states. \n",
    "\n",
    "One might argue that the black cells are not states at all, but for the sake of convenience in the code implementation, these cells have been numbered as states 6 and 7. In your computations, you can treat these values as terminal states and the values of these states will therefore be always 0. Strictly speaking, we never take any actions in a terminal state, but in our visualizations we assume the default action Up for these states, which is an arbitrary choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7dc44a7d986178a5d2a3e2a818b5da46",
     "grade": false,
     "grade_id": "cell-248f09eb0d87487b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.4 Markov Decision Process\n",
    "Formally, we describe our problem as a Markov decision process (MDP). An MDP consists of the following tuple: $$(\\cal S, \\cal A, \\cal P, \\cal R, \\gamma),$$ \n",
    "where $\\cal S$ is the set of all states, and $S_{t}$ is the state of the agent at time $t$,\\\n",
    "$\\cal A$ is the set of all actions and $A_{t}$ is the action taken in $S_{t}$ by the agent,\\\n",
    "$\\cal P$ is the transition model, defined as $\\cal{P}_{ss'}^a$ $=\\mathbb{P}[S_{t+1}=s'\\big|S_t=s,A_t=a]$, which gives the probability to transition from state $s$ to $s'$ given action $a$,\\\n",
    "$\\cal R$ is the reward model, defined as $\\cal{R}_s^a$ $= \\mathbb{E}[R_{t+1} | S_t = s, A_t=a]$, which gives the expected reward for taking the action $a$ from the state $s$,\\\n",
    "$\\gamma$ is the discount factor used to discount future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d139a53c6a3c411e4597cb2e26415fa",
     "grade": false,
     "grade_id": "cell-fd553822a47ca613",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 1.4.1: (4 Points)**\n",
    "Suppose the agent starts in the top left corner of the grid world with a deterministic policy, that is, at the state $s=0$, and takes the action $a=\\text{Right} \\ $ five times until the episode ends. Write the reward $R_t$ and the return $G_t$ for every time step. You can assume that the agent starts in $s=0$ at $t=0$ such that it reaches the terminal state at $t=5$, and assume $\\gamma = 0.9$. (Note that we only receive rewards at the time steps 1, 2, 3, 4 and 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4c0a12df0810e0b67fd38d1fe0c9243",
     "grade": true,
     "grade_id": "cell-f6c2f9066e4e2351",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26b5c674c237ef3befcf2c64c50e46bc",
     "grade": false,
     "grade_id": "cell-fcdf8f9ecc047503",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Section 2: Model-based Reinforcement Learning\n",
    "\n",
    "In this section, we explore model-based reinforcement learning algorithms including Policy Evaluation, Policy Iteration and Value Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e364e4291a02e0f651411f398b6bac33",
     "grade": false,
     "grade_id": "cell-ae15c7ee92fe0864",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Policy Evaluation\n",
    "The ability to evaluate a policy is essential in reinforcement learning. Formally, this means that we compute the value function $v_{\\pi}(s)$ (or the action-value function $q_{\\pi}(s,a)$) for a given policy $\\pi$. We will now study how this can be done in a model-based setting, that is, when the models in our Markov decision problem are known. \n",
    "\n",
    "Remark: The policy evaluation algorithm is also known as the Iterated policy evaluation algorithm in, e.g., the book 'Reinforcement Learning: An Introduction' by Sutton and Barto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5037ebe47e113c1d53e6db3fba0d38d",
     "grade": false,
     "grade_id": "cell-9431a47b8886bd42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 2.1.1: (5 Points)** Assume we have a deterministic transition model such that if the agent takes an action $a$, it will move in that direction with probability 1. You are given a stochastic policy $\\pi$ shown in the figure below, that takes each of the four actions with equal probability (uniformly random) in all states, $$\\pi(a \\mid s) = 0.25, \\quad \\text{for all states} \\ s \\in {\\cal S} \\ \\text{and actions} \\  a \\in {\\cal A}.$$ Assume $\\gamma = 0.9$.\n",
    "\n",
    "![title](./random_policy.png)\n",
    "\n",
    "Compute the state-value updates by hand for the first three iterations of the Policy Evaluation algorithm, that is, the state-values $\\hat{v}_{\\pi}^{(1)}(s)$, $\\hat{v}_{\\pi}^{(2)}(s)$ and $\\hat{v}_{\\pi}^{(3)}(s)$ for all states $s$ under the policy $\\pi$. Assume $\\gamma = 0.9$ and $\\hat{v}_{\\pi}^{(0)}(s) = 0$ for all states $s$.\n",
    "\n",
    "$\\hat{v}_{\\pi}^{(k)}(s)$:\n",
    "\n",
    "|$\\hat{v}_{\\pi}^{(k)}(0)$|$\\hat{v}_{\\pi}^{(k)}(1)$|$\\hat{v}_{\\pi}^{(k)}(2)$|$\\hat{v}_{\\pi}^{(k)}(3)$|$\\hat{v}_{\\pi}^{(k)}(4)$|\n",
    "|------------------------|------------------------|------------------------|------------------------|------------------------|\n",
    "|$\\hat{v}_{\\pi}^{(k)}(5)$|$\\hat{v}_{\\pi}^{(k)}(6)$|$\\hat{v}_{\\pi}^{(k)}(7)$|$\\hat{v}_{\\pi}^{(k)}(8)$|$\\hat{v}_{\\pi}^{(k)}(9)$|\n",
    "|$\\hat{v}_{\\pi}^{(k)}(10)$|$\\hat{v}_{\\pi}^{(k)}(11)$| $\\hat{v}_{\\pi}^{(k)}(12)$|$\\hat{v}_{\\pi}^{(k)}(13)$|$\\hat{v}_{\\pi}^{(k)}(14)$|\n",
    "\n",
    "Note $\\hat{v}_{\\pi}^{(k)}(6)$ and $\\hat{v}_{\\pi}^{(k)}(7)$ are always 0 for all iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b131760d18c3ae9d462c932063c1c0f5",
     "grade": true,
     "grade_id": "cell-0998bdeaf5937e11",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Answer:\n",
    "\n",
    "# Write your values in the following format for the variables v_0, v_1, v_2, v_3\n",
    "# v_k = [[v(0) , v(1) , v(2) , v(3) , v(4) ],\n",
    "#        [v(5) , v(6) , v(7) , v(8) , v(9) ],\n",
    "#        [v(10), v(11), v(12), v(13), v(14)]]\n",
    "# After filling in all the values, run this cell to see your values in a table format\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Code to print the values in a nice format\n",
    "import numpy as np\n",
    "\n",
    "def print_value_table(values):\n",
    "    v= np.ravel(values)\n",
    "    if (len(v)==16):\n",
    "        v = v[:-1]\n",
    "    assert len(v) == 15, \"Some values are missing from your input. Make sure you have entered values for all states.\"\n",
    "    indecies = np.arange(0, 15, 5)\n",
    "    hor_delimiter = \"___________________________________________________\"\n",
    "    print('State Values: ')\n",
    "    print(hor_delimiter)\n",
    "    for i in indecies:\n",
    "        print(\"| \", '%-6.3f' % v[i] , \"| \", '%-6.3f' % v[i+1] , \"| \", '%-6.3f' % v[i+2] ,\n",
    "              \"| \", '%-6.3f' % v[i+3] , \"| \", '%-6.3f' % v[i+4] , \"| \")\n",
    "        print(hor_delimiter) \n",
    "\n",
    "\n",
    "print(\"\\n v_0:\")\n",
    "print_value_table(v_0)\n",
    "print(\"\\n v_1:\")\n",
    "print_value_table(v_1)\n",
    "print(\"\\n v_2:\")\n",
    "print_value_table(v_2)\n",
    "print(\"\\n v_3:\")\n",
    "print_value_table(v_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8947a73a3139753567d7cee0bc9ea544",
     "grade": false,
     "grade_id": "cell-2b275f147d2ab91e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 2.1.2: (2 Points)** Describe the steps you took to arrive at the value $\\hat{v}_{\\pi}^{(3)}(8)$ for the state $s = 8$ in the third iteration. You do not have to explain how $\\hat v_{\\pi}^{(2)}(s)$ was computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f6411538e1772f154f8d14df65b6de1",
     "grade": true,
     "grade_id": "cell-cd6e7b8a891317de",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91d11bbdb82a8333f188f6c6ccb87214",
     "grade": false,
     "grade_id": "cell-744899bfd43b3bc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Policy Iteration\n",
    "To find the optimal policy $\\pi_*$, when the models in our MDP are given, we can use policy iterations, where we iterate between the two steps: \n",
    "\n",
    "1. Policy evaluation\n",
    "2. Policy improvement\n",
    "\n",
    "\n",
    "In the **Policy Evaluation**, we evaluate our current policy $\\pi$ and find $v_{\\pi}(s)$.\n",
    "\n",
    "In the **Policy Improvement**, we update our policy to the greedy policy with respect to the state values $v_{\\pi}(s)$ obtained in the Policy Evaluation step $$\\pi \\leftarrow \\pi_{greedy}(v_{\\pi}(s)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a367657bcbe82b6dc86396066d11c08f",
     "grade": false,
     "grade_id": "cell-bcaefc1ccf075b6e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 2.2.1: (1 Point)** If we continue performing iterations of Policy Evaluation in Task 2.1.1, after iteration 49, our values converge to the following values for $v_{\\pi}(s)$:\n",
    "\n",
    "| $0.023$  | $0.059$  | $0.121$  | $0.237$  | $1$      |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| $-0.003$  | $0$      | $0$      | $-0.303$ | $-1$    |\n",
    "| $-0.031$ | $-0.072$ | $-0.145$ | $-0.282$ | $-0.525$ |  \n",
    "\n",
    "\n",
    "Find the greedy policy $\\pi = greedy(v_{\\pi}(s))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click this cell and use the following grid and symbols to write the policy you found in the answer..\n",
    "\n",
    "Actions = ↑, →, ↓, ←\n",
    "\n",
    "| ↑  | ↑  | ↑  | ↑  | ↑  |\n",
    "|----|----|----|----|----|\n",
    "| ↑  | ↑  | ↑  | ↑  | ↑  |\n",
    "| ↑  | ↑  | ↑  | ↑  | ↑  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "209d09ecf7b065b29fca0e4389d91315",
     "grade": true,
     "grade_id": "cell-865e9716a5852185",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b99de772355c688f4d8305881918ff35",
     "grade": false,
     "grade_id": "cell-41e88474e1466613",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 2.2.2: (4 Points)** Let $\\pi$ be the policy that you obtained in the solution to Task 2.2.1. Compute $v_{\\pi}(s)$. Note that both the MDP and the policy are now deterministic, which means that the values can be obtained directly (the rewards are deterministic functions of the states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39c8fa566312ddf1171641bb96d38776",
     "grade": true,
     "grade_id": "cell-aa3b153b28de3766",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Answer:\n",
    "\n",
    "# Write your state values in the following format.\n",
    "# v = [[v(0) , v(1) , v(2) , v(3) , v(4) ],\n",
    "#        [v(5) , v(6) , v(7) , v(8) , v(9) ],\n",
    "#        [v(10), v(11), v(12), v(13), v(14)]]\n",
    "# After filling in all the values, run this cell to see your values in a table format\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Code to print the values in a nice format\n",
    "print(\"\\n v:\")\n",
    "print_value_table(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed3d2ce8107e391a4ba5762d9812c3b9",
     "grade": false,
     "grade_id": "cell-2bba703bd7db7710",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 2.2.3: (1 Points)**  Compute the greedy policy with respect to the state values that you obtained in Task 2.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click this cell and use the following grid and symbols to write the policy you found in the answer..\n",
    "\n",
    "Actions = ↑, →, ↓, ←\n",
    "\n",
    "| ↑  | ↑  | ↑  | ↑  | ↑  |\n",
    "|----|----|----|----|----|\n",
    "| ↑  | ↑  | ↑  | ↑  | ↑  |\n",
    "| ↑  | ↑  | ↑  | ↑  | ↑  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "037ddea860a5d1bed84ee90c13ba7ec7",
     "grade": true,
     "grade_id": "cell-62c88af7455ffb85",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b5cf670bebcf3420ee10212146e5cce",
     "grade": false,
     "grade_id": "cell-ab80df325256cf89",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "###  2.3 Value Iteration\n",
    "**Value Iteration** is another algorithm used for model-based reinforcement learning. Value iteration is an iterative algorithm used to compute the optimal value function $\\hat{v}_*(s)$. Each iteration starts with a guess of what the value function is and then uses the Bellman optimality equations to improve this guess iteratively. We can describe one iteration of the algorithm as\n",
    "\n",
    "$\n",
    "\\textbf{For all} ~ s \\in {\\cal S}:\\qquad  \\\\\n",
    "\\quad \\hat{v}_{k+1}(s) = \\underset{a \\in {\\cal A}}{\\text{max}}~ \\left( \\mathcal{R}_s^a + \\gamma \\underset{{s'\\in \\mathcal{S}}}{\\sum} \\mathcal{P}_{ss'}^a \\cdot \\hat{v}_k(s') \\right)\n",
    "$\n",
    "\n",
    "where $\\mathcal{P}_{ss'}^a=\\mathbb{P}[S'=s'\\big|S=s,A=a]$ is the probability to transition from state $s$ to $s'$ given action $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8cc928accc855994b295b5908c474b7",
     "grade": false,
     "grade_id": "cell-4681b7a29abb5a48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the next task, we will implement the value iteration algorithm, and find the optimal policy $\\pi_*$ for our grid world. We have defined an MDP Python class and some helper functions, which are described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bf4a4b464425dc74727ef8aca3f2110",
     "grade": false,
     "grade_id": "cell-74497ad9b13e8362",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### The MDP Python class\n",
    "The Markov Decision Process you will work with is defined in `gridworld_mdp.py`. In the implementation, the actions are represented by integers as, Up = 0, Left = 1, Down = 2, and Right = 3.\n",
    "To interact with the MDP, you need to instantiate an object as: \n",
    "\n",
    "\n",
    "```python\n",
    "mdp = GridWorldMDP()\n",
    "```\n",
    "\n",
    "At your disposal there are a number of instance-functions implemented for you, and presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "636b8c5ea0bb7d3b4798564dfc7d580d",
     "grade": false,
     "grade_id": "cell-21e5d7b3d3083cd6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from gridworld_mdp import *\n",
    "import numpy as np\n",
    "\n",
    "help(GridWorldMDP.get_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "620f4674b0caadf475b4889bd8747b62",
     "grade": false,
     "grade_id": "cell-9706322eb34e16db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The constructor\n",
    "help(GridWorldMDP.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9b14ba756ee79811ccba63e1e51f14a",
     "grade": false,
     "grade_id": "cell-38d3ab6fb24c1af8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "help(GridWorldMDP.get_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0b98cbe90baa640578ed1991b4e3501",
     "grade": false,
     "grade_id": "cell-ecb00397472a5faa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "help(GridWorldMDP.state_transition_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40120f95b5767060a8ca55455cf5d485",
     "grade": false,
     "grade_id": "cell-aa8e1498649053a5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "help(GridWorldMDP.reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43698ce0987456ff4859dccfb26cc59c",
     "grade": false,
     "grade_id": "cell-c1408cc9707dd7f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "To visualize the state values you can use the function **print_value_table(values)** from task 2.1.1. We also provide a helper function for visualizing the policies you obtain below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ec0becf9428e5b228fa2cb6968816ae",
     "grade": false,
     "grade_id": "cell-b754590784e24eb1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Function for printing a policy pi\n",
    "def print_policy(pi):\n",
    "    print('\\n Policy: ')\n",
    "    indencies = np.arange(1, 16)\n",
    "    txt = '| '\n",
    "    hor_delimiter = '---------------------'\n",
    "    print(hor_delimiter)\n",
    "    for a, i in zip(pi, indencies):\n",
    "        txt += mdp.act_to_char_dict[a] + ' | '\n",
    "        if i % 5 == 0:\n",
    "            print(txt + '\\n' + hor_delimiter)\n",
    "            txt = '| '      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "842be43906819eecbe30d63fa3c180d1",
     "grade": false,
     "grade_id": "cell-87e02763b23fe1f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 2.3.1: (3 Points)** Now it's time for you to implement your own version of value iteration to solve for the optimal value function $v_*(s)$, and the optimal policy $\\pi_*(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9847e4f97ab4b8b19a719c236fb7a9ce",
     "grade": true,
     "grade_id": "cell-f58e438ec8dcb5d1",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Answer:\n",
    "def value_iteration(gamma, mdp):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        V - state value table, numpy array of shape (16,)\n",
    "        pi - greedy policy table, numpy array of shape (16,)\n",
    "    \"\"\"\n",
    "    V = np.zeros([16]) # state value table\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2df5724f72465b7404fe2e0837bfb9f7",
     "grade": false,
     "grade_id": "cell-99c149095318adac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run your implementation for the deterministic version of our MDP. Check whether the optimal values and the optimal policy look appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a8722a6c33a96991f1091e2418def51",
     "grade": false,
     "grade_id": "cell-bd495acfe33d405f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mdp = GridWorldMDP(trans_prob=1.)\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "843f1c2dcaf84325ebb5fff8404380be",
     "grade": false,
     "grade_id": "cell-5a24214a0645d4b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Once the optimal values and the optimal policy look appropriate, run it for the stochastic case, such that when the agent takes an action $a$ there is 0.8 probability that it will move in the intended direction and 0.2 probability that it will move in one of the orthogonal directions to the intended with equal probability. Use $\\gamma = .99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a75533b7ea080eeb46e3a5ba2cdef80e",
     "grade": false,
     "grade_id": "cell-c6d0282ee295bb85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run for stochastic MDP, gamma = .99\n",
    "mdp = GridWorldMDP(trans_prob=.8)\n",
    "v, pi = value_iteration(.99, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c9a85313df31cfcddcc808f7f6c3c2a",
     "grade": false,
     "grade_id": "cell-b80f5f5b9d1398a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 2.3.2: (2 Points)** For the Stochastic case, does the policy that the algorithm found look reasonable? Provide a brief description for your answer. In particular, what's the policy for state $s=8$? Is that a good idea? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12691470ab1f2881c2087ad32395b3a5",
     "grade": true,
     "grade_id": "cell-daff5655fe78f131",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c94489bd5a5a7608e4fa04705176796",
     "grade": false,
     "grade_id": "cell-d4840da19cbbb63a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Test your implementation using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5071817f08d1145df1c5095dab2e7dc",
     "grade": false,
     "grade_id": "cell-f89a5e7709d41efc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_value_iteration(v, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a2a2376284478e5ff87e8fc45681df1",
     "grade": false,
     "grade_id": "cell-32b52b966ea12de5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run value iteration for the same scenario as above, but now with $\\gamma=.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d1713f8344b7574447b39700f4a5e61",
     "grade": false,
     "grade_id": "cell-3f797c0f704c2394",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run for stochastic MDP, gamma = .9\n",
    "mdp = GridWorldMDP(trans_prob=.8)\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "311203f4bf618aa65d08853156ac19c4",
     "grade": false,
     "grade_id": "cell-9192d61af754d47b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 2.3.3: (1 Point)** Do you notice any difference between the greedy policy for the two different discount factors. If so, what's the difference, and why do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f965adadab673ae6af1a4d4ee72ae914",
     "grade": true,
     "grade_id": "cell-1a675e7574dce1d5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ba27dc1013c12f9b7e53be6f531bf71",
     "grade": false,
     "grade_id": "cell-01feb7e04644407c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Section 3: Model-free Reinforcement Learning \n",
    "In most of Reinforcement Learning (RL) problems, we do not have access to the transition/dynamics model and the reward model. To evaluate a policy in such problems, we use model-free policy evaluation algorithms which rely on interactions with the environment to estimate the state and action values, e.g., Monte Carlo policy evaluation, TD-learning. To find the optimal values and optimal policy in a model-free RL problem, algorithms like Monte Carlo policy iterations, SARSA and Q-learning can be used. In this section, we cover the Q-learning algorithm.\n",
    "\n",
    "### 3.1 Q-learning\n",
    "\n",
    "In the previous section, you solved for $v_*(s)$ and the greedy policy $\\pi_*(s)$, with the entire model of the MDP being available to you. Now we consider the model-free case, where we do not have the transition model or the reward model available to us. \n",
    "\n",
    "In this section, you will implement the Q-learning algorithm, an alternative that learns $q$-values from experience, without the need of a model.\n",
    "\n",
    "#### Q-learning algorithm\n",
    "$\n",
    "\\text{Initialize}~\\hat{q}(s,a), ~ \\forall~ s \\in {\\cal S},~ a~\\in {\\cal A} \\\\\n",
    "\\textbf{Repeat}~\\text{(for each episode):}\\\\\n",
    "\\quad \\text{Initialize}~s\\\\\n",
    "\\qquad \\textbf{Repeat}~\\text{(for each step in episode):}\\\\\n",
    "\\qquad\\quad \\text{Choose $a$ from $s$ using policy derived from $\\hat{q}$ (e.g., $\\epsilon$-greedy)}\\\\\n",
    "\\qquad\\quad \\text{Take action a, observe r, s'}\\\\\n",
    "\\qquad\\quad \\hat{q}(s,a) \\leftarrow \\hat{q}(s,a) + \\alpha \\left(r + \\gamma~\\underset{a'}{\\text{max}}~\\hat{q}(s',a') - \\hat{q}(s,a) \\right) \\\\\n",
    "\\qquad\\quad s \\leftarrow s' \\\\\n",
    "\\qquad \\text{Until s is terminal}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "151fe213a584c690f6046c0af32bd1d4",
     "grade": false,
     "grade_id": "cell-bbe7f99d1cc4e6af",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 3.1.1: (2 Points)**  Implement an $\\epsilon$-greedy policy\n",
    "\n",
    "The goal of the Q-learning algorithm is to find the optimal policy $\\pi_*$, by estimating the state action value function under the optimal policy, i.e., $\\hat{q}_*(s, a)$. From $\\hat{q}_*(s,a)$, the agent can follow $\\pi_*$ by choosing the action that yields the largest expected value for each state, i.e., $\\text{argmax}_a~\\hat{q}_*(s, a)$. However, when training a Q-learning model, the agent typically follows another policy to explore the environment (instead of the one that maximizes the current Q-values). In reinforcement learning this is known as off-policy learning. \n",
    "\n",
    "Your task is to implement a widely popular exploration policy, known as  the $\\epsilon$-greedy policy, in the cell below.\n",
    "\n",
    "An $\\epsilon$-greedy policy should:\n",
    "* with probability $\\epsilon$ take an uniformly-random action.\n",
    "* otherwise choose the best action according to the estimated state action values.\n",
    "\n",
    "*Hint:* The $\\epsilon$-greedy policy can be implemented extra elegantly by calculating the actual resulting sampling distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "382cb07c5f402ad975be60733d1dbfc6",
     "grade": true,
     "grade_id": "cell-48c826a87791fb56",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "583492c65cb090c181c66b42c2a51eff",
     "grade": false,
     "grade_id": "cell-6d33489b428b1179",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a884d531a9dbbfeebaac3af3d5d309e6",
     "grade": false,
     "grade_id": "cell-80bd577e278ec0b0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "mdp = GridWorldMDP()\n",
    "\n",
    "# Test shape of output\n",
    "actions = mdp.get_actions()\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "actions = [i for i in range(10)]\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "\n",
    "# Test for greedy actions\n",
    "for a in actions:\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[a] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, 0)\n",
    "    assert np.allclose(foo, eps_greedy, rtol=1e-03), \"policy is not greedy\"\n",
    "\n",
    "# Test for uniform distribution, when eps=1\n",
    "eps_greedy = eps_greedy_policy(foo, 1)\n",
    "assert all(math.isclose(p, eps_greedy[0], rel_tol=1e-03) for p in eps_greedy),\\\n",
    "    \"policy does not return a uniform distribution for eps=1\"\n",
    "assert math.isclose(np.sum(eps_greedy), 1.0, rel_tol=1e-03), \"policy distribution is not normalized\"\n",
    "\n",
    "\n",
    "print('Test passed, good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95f7aae73036b3bf8a52ca0069e388a9",
     "grade": false,
     "grade_id": "cell-1dccaeebe5a41325",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now it's time to actually implement the Q-learning algorithm. Unlike the Value iteration where there are no direct interactions with the environment, the Q-learning algorithm builds up its estimates by interacting and exploring the environment. \n",
    "\n",
    "To enable the agent to explore the environment a set of helper functions are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02bb92b833a3dc4e535cda13a4a1fae4",
     "grade": false,
     "grade_id": "cell-881edd2be439489e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "help(GridWorldMDP.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "589834a8662125b7792560c134990d91",
     "grade": false,
     "grade_id": "cell-061e7670ebd7b35c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "help(GridWorldMDP.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a860d6d33852d2a6df11fb13b3f628a",
     "grade": false,
     "grade_id": "cell-15fa6bbf763cdc6f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 3.1.2: (4 Points)** Implement your version of Q-learning in the cell below. \n",
    "\n",
    "**Hint:** It might be useful to study the pseudocode provided above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "574c8a5af1d867ea6b0455fe7f0a5b6a",
     "grade": true,
     "grade_id": "cell-4200fb8e53eee9e9",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "def q_learning(eps, gamma, mdp):\n",
    "    q_hat = np.zeros([16, 4]) # state action value table\n",
    "    pi = np.zeros([16]) # greedy policy table\n",
    "    alpha = .01\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return pi, q_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7826f01b01a42ef8f3bf3df0d1bddf9a",
     "grade": false,
     "grade_id": "cell-b48032d234ecb11d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run Q-learning with for the stochastic MDP with $\\epsilon = 1, \\gamma=0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42cbf84b1184e481123afffe29fd7d90",
     "grade": false,
     "grade_id": "cell-0464324eb2e2bf9c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mdp = GridWorldMDP()\n",
    "pi, q_hat = q_learning(1, .99, mdp)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bc5026e570acf5162847f04d4b2daf3",
     "grade": false,
     "grade_id": "cell-a424df8abe557f2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Test your implementation by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a6913660c5ca98d01c502463b5107bc",
     "grade": false,
     "grade_id": "cell-e2832d3538099d67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_q_learning(q_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d769339e7558ff13cbf68f638b558200",
     "grade": false,
     "grade_id": "cell-d3623c3f5c170bd4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run Q-learning with for the stochastic MDP with $\\epsilon = 0, \\gamma=0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "456c220408348cd213624d95dc8d6c3e",
     "grade": false,
     "grade_id": "cell-1c095409c30320d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mdp = GridWorldMDP()\n",
    "pi, q_hat = q_learning(0, .99, mdp)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1868ed5b07bbe5c7246a764094405e6",
     "grade": false,
     "grade_id": "cell-d3383d13bae73e68",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 3.1.3: (2 Points)** You ran your implementation with $\\epsilon$ set to both 0 and 1. Did both values yield the same solution? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5f95e5de93e2d67454fc6bbaebac47c",
     "grade": true,
     "grade_id": "cell-9d0d5871e2c0d97c",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7aabca9753fdc79d2e5adbf1ce4b5f13",
     "grade": false,
     "grade_id": "cell-ae2a001335118014",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Section 4: Deep Q-learning \n",
    "In this section, you will implement a deep Q-learning network (DQN) to solve one of the problems of the OpenAI gym. Before we get to the implementation, let's first review DQN theory.\n",
    "\n",
    "As we saw in the video lectures, using a neural network as a state action value approximator is a great idea. However, if one tries to use this approach with Q-learning, it's very likely that the optimization will be very unstable. To remediate this, two main ideas are used.\n",
    "- First, we use experience replay, in order to decorrelate the experience samples we obtain when exploring the environment.\n",
    "- Second, we use two networks instead of one, in order to fix the optimization targets. Each of these ideas are explained below.\n",
    "\n",
    "### 4.1 Experience replay\n",
    "Since Q-learning is an off-policy algorithm, we may collect data by navigating in the environment using some choice of behavioral / exploratory policy $\\mu$ (e.g. $\\epsilon$-greedy), while still learning the $\\hat{q}$ values for an optimal policy. Experience replay exploits this even further, and re-uses old data, which was collected using whatever exploratory policy we used at that moment (e.g. $\\epsilon$-greedy w.r.t. to that data approximate $\\hat{q}$ values).\n",
    "\n",
    "Except for re-using already collected data, another advantage of experience replay is that it allows us to decorrelate the data, by sampling experience from very different parts of the environment, rather than using the samples for training in the order they were collected when walking around in the environment.\n",
    "\n",
    "Decorrelating the data is essential for training neural networks.\n",
    "\n",
    "### 4.2 Fixing the optimization target\n",
    "For a given minibatch sampled from the replay buffer, we'll optimize the weights of only one of the networks (commonly denoted as the \"online\" network), using the gradients w.r.t a loss function. This loss function is computed as the mean squared error between the current action values, computed according to the **online** network, and the Q targets, computed using the other, **offline network** (which we'll also refer to as the fixed network or target network).\n",
    "\n",
    "That is, the loss function is \n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left(\\hat{q}(s_i,a_i; \\theta\\right) - y_i)^2~,$$\n",
    "\n",
    "where $N$ is the number of samples in your minibatch, $\\hat{q}(s,a;\\theta)$ is the state action value estimate, according to the online network (with parameters $\\theta$), and $y_t$ is the Q target, computed as\n",
    "\n",
    "$$ y_i = r_i +  \\gamma ~\\underset{a}{\\text{max}}~\\hat{q}(s_i', a; \\theta^-)~, $$\n",
    "\n",
    "where $\\hat{q}(s', a;\\theta^-)$ is the action value estimate, according to the offline network (with parameters $\\theta^-$).\n",
    "\n",
    "Finally, so that the offline parameters are also updated, we periodically copy the parameters from the online to the offline network.\n",
    "\n",
    "### 4.3 Training loop essentials\n",
    "The following key components are repeated for every time step $t$ in an episode:\n",
    "1. Sample an action $a_t$ from the $\\epsilon$-greedy policy w.r.t. the current estimated $\\hat{q}$ values (online network), and execute the action in the environment.\n",
    "1. The current transition $(s_t, a_t, r_{t+1}, s_{t+1})$, which refers to a sequence of the current state, action, immediate reward and next state, is stored in the replay buffer.\n",
    "1. An entire mini-batch of transitions is sampled from the replay buffer, and a gradient step is taken to improve the online network.\n",
    "\n",
    "### 4.4 Target notation\n",
    "\n",
    "Several different optimization targets which are estimated with some $q$ function are often jointly referred to as \"TD targets\".\n",
    "We strive to be consistent and separate on-policy \"TD(0) targets\" and off-policy \"Q targets\" but in other places this distinction may be less clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e11a4bdad924c39cd0ed3c9dddf6c25",
     "grade": false,
     "grade_id": "cell-b37fc5ebe369b45a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.5 Environment\n",
    "\n",
    "The problem you will solve for this task is the inverted pendulum problem. \n",
    "On [Open AIs environment documentation](https://gym.openai.com/envs/CartPole-v0) , the following description is provided:\n",
    "\n",
    "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time step that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.*\n",
    "\n",
    "Furthermore, the episode will automatically end if 200 steps are reached, as explained [here](https://github.com/openai/gym/wiki/CartPole-v0#episode-termination).\n",
    "\n",
    "![title](./cartpole.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74de453978cf5a4ced5167cc4bed7cc9",
     "grade": false,
     "grade_id": "cell-920f9802c12678ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Run the cell below to see a video illustration of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "185ad818161c47503d9eea9d466d890f",
     "grade": false,
     "grade_id": "cell-bc01c34a37f53b30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('46wjA6dqxOM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e47b9a85f06e4fa121afde23b36af2f2",
     "grade": false,
     "grade_id": "cell-181515edc2a1f7a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.6 Implementation\n",
    "We'll solve this task using a DQN. Most of the code is provided for you, in the file **dqn_model.py**. This file contains the implementation of a neural network, which is described in the table below (feel free to experiment with different architectures).\n",
    "\n",
    "|Layer 1: units, activation | Layer 2: units, activation | Layer 3: units, activation | Cost function |\n",
    "|---------------------------|----------------------------|----------------------------|---------------|\n",
    "| 100, ReLu                 | 60, ReLu                   | number of actions, linear | MSE           |\n",
    "\n",
    "There are however a few key parts missing from the code, that are to be implemented in the following three functions:\n",
    "- `calc_q_and_take_action`\n",
    "- `calculate_q_targets`\n",
    "- `sample_batch_and_calculate_loss`\n",
    "\n",
    "These will then be called from the function `train_loop_dqn`, which runs the main loop for training the model in the cart-pole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "299fba3a16b12d807ee6e38ad27fba82",
     "grade": false,
     "grade_id": "cell-8ef4581670ad2682",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 4.6.1: (2 Points)** Calculate Q-values for the current state, and decide on which action to take. Use an epsilon-greedy behavioral policy, and feel free to re-use the `epsilon_greedy_policy` function that you defined for the Q-learning part.\n",
    "\n",
    "This function will be used to control the agent's behavior in the environment, but the actual training will be done later, for entire mini-batches sampled from the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91ba6036c9d7f1ff876a653dc376d3a0",
     "grade": true,
     "grade_id": "cell-5ef2ca4ae83dd031",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Answer:\n",
    "def calc_q_and_take_action(dqn, state, eps):\n",
    "    '''\n",
    "    Calculate Q-values for current state, and take an action according to an epsilon-greedy policy.\n",
    "    Inputs:\n",
    "        dqn   - DQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        state  - Current state. Numpy array, shape (1, num_states).\n",
    "        eps    - Exploration parameter.\n",
    "    Returns:\n",
    "        q_online_curr   - Q(s,a) for current state s. Numpy array, shape (1, num_actions) or  (num_actions,).\n",
    "        curr_action     - Selected action (0 or 1, i.e., left or right), sampled from epsilon-greedy policy. Integer.\n",
    "    '''\n",
    "    # FYI:\n",
    "    # dqn.online_model & dqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return q_online_curr, curr_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ec3ac08e53caa4988eb91bef6346773",
     "grade": false,
     "grade_id": "cell-ae647de789982b2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 4.6.2: (2 Points)** For following task, you will calculate the temporal difference target used for the loss in the deep Q-learning algorithm. Your implementation should adhere to the equation defined above for the Q target of DQNs, with one exception: when s' is terminal, the Q target for it should simply be $ Y_i = r_i$. Why is this necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "812801ece84232453950e6a029d7b402",
     "grade": true,
     "grade_id": "cell-74ac47cb855f34cd",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "532ff6b7e9535b4b61b32ba0b45a361a",
     "grade": false,
     "grade_id": "cell-26a12456d7c70776",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 4.6.3: (1 Points)** Implement your function which calculates Q target in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bc159c47693822120b3664917952a77",
     "grade": true,
     "grade_id": "cell-897feec9998a73d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "def calculate_q_targets(q1_batch, r_batch, nonterminal_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the Q target used for the loss\n",
    "    : param q1_batch: Batch of q_hat(s', a) from target network. FloatTensor, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards. FloatTensor, shape (N,)\n",
    "    : param nonterminal_batch: Batch of booleans, with False elements if state s' is terminal and True otherwise. BoolTensor, shape (N,)\n",
    "    : param gamma: Discount factor, float.\n",
    "    : return: Q target. FloatTensor, shape (N,)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "198cb1c800b2c103c9fcdfeb9fd45eda",
     "grade": false,
     "grade_id": "cell-b4a77bdb942ce919",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your implementation by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d83145c0697d0ac4b58857c62c33798a",
     "grade": false,
     "grade_id": "cell-bd5e9c36540130c0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import dqn_model\n",
    "dqn_model.test_calculate_q_targets(calculate_q_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65b909e6ec3f605041ddc56a6976f370",
     "grade": false,
     "grade_id": "cell-90601693d1f77faf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 4.6.4: (2 Points)** Use the online & offline Q-networks to calculate the Q-values for a minibatch. These will then be used to calculate the mini-batch loss by the end of the function.\n",
    "\n",
    "\n",
    "You will need to define two tensors:\n",
    "- `q_online_curr`: $\\hat{q}(s,a; \\theta), \\ \\forall a$\n",
    "- `q_offline_next`: $\\hat{q}(s',a; \\theta^-), \\ \\forall a$\n",
    "\n",
    "Take great care to make sure gradient computation is enabled / disabled where it should. `torch.no_grad()` is your friend here (see [Pytorch docs](https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af9b19f490c8688898a4e4fc1b7521f1",
     "grade": true,
     "grade_id": "cell-8e9e5656d707e8ea",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "def sample_batch_and_calculate_loss(dqn, replay_buffer, batch_size, gamma):\n",
    "    '''\n",
    "    Sample mini-batch from replay buffer, and compute the mini-batch loss\n",
    "    Inputs:\n",
    "        dqn          - DQN model. An object holding the online / offline Q-networks, and some related methods.\n",
    "        replay_buffer - Replay buffer object (from which samples will be drawn)\n",
    "        batch_size    - Batch size\n",
    "        gamma         - Discount factor\n",
    "    Returns:\n",
    "        Mini-batch loss, on which .backward() will be called to compute gradient.\n",
    "    '''\n",
    "    # Sample a minibatch of transitions from replay buffer\n",
    "    curr_state, curr_action, reward, next_state, nonterminal = replay_buffer.sample_minibatch(batch_size)\n",
    "\n",
    "    # FYI:\n",
    "    # dqn.online_model & dqn.offline_model are Pytorch modules for online / offline Q-networks, which take the state as input, and output the Q-values for all actions.\n",
    "    # Input shape (batch_size, num_states). Output shape (batch_size, num_actions).\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    q_target = calculate_q_targets(q_offline_next, reward, nonterminal, gamma=gamma)\n",
    "    loss = dqn.calc_loss(q_online_curr, q_target, curr_action)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abea5c2041854bab1925a83b53b7802a",
     "grade": false,
     "grade_id": "cell-401656fa71e3f227",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Test your implementation by trying to solve the reinforcement learning problem for the Cartpole environment. The `train_loop_dqn` function defined below will be called later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "658df43c8e6ac3bf2756b4126dd7230c",
     "grade": false,
     "grade_id": "cell-f3d381be408141e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from dqn_model import DeepQLearningModel, ExperienceReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU should be enough, but feel free to play around with this if you want to.\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60af8b46abf7cec4e11752f3d9185e2d",
     "grade": false,
     "grade_id": "cell-a9e3d9e027c537be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_loop_dqn(dqn, env, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not finish_episode:\n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "\n",
    "            # Take one step in environment. No need to compute gradients,\n",
    "            # we will just store transition to replay buffer, and later sample a whole batch\n",
    "            # from the replay buffer to actually take a gradient step.\n",
    "            q_online_curr, curr_action = calc_q_and_take_action(dqn, state, eps)\n",
    "            q_buffer.append(q_online_curr)\n",
    "            new_state, reward, finish_episode, _ = env.step(curr_action) # take one step in the evironment\n",
    "            new_state = new_state[None,:]\n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            nonterminal_to_buffer = not finish_episode or steps == 200\n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                loss = sample_batch_and_calculate_loss(dqn, replay_buffer, batch_size, gamma)\n",
    "                dqn.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                dqn.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    dqn.update_target_network()\n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "\n",
    "        print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer))))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dbdd4e0e629188932bae9d37d05253c",
     "grade": false,
     "grade_id": "cell-75c84628ce999711",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following cell performs the actual training. \n",
    "\n",
    "A working implementation should start to improve after 500 episodes. An episodic reward of around 200 is likely to be achieved after 800 episodes for a batchsize of 128, and 1000 episodes for a batchsize of 64.\n",
    "\n",
    "**Note:** The `enable_visualization` flag controls whether a visualization of the cart-pole environment will be plotted. In many environments, this is however not working properly, for which reason it is disabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_episodes = 1200 \n",
    "batch_size = 128\n",
    "gamma = .94\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "dqn = DeepQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)\n",
    "\n",
    "# Train\n",
    "R, R_avg = train_loop_dqn(dqn, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52dee2317f03bced29c85c28462d4ad5",
     "grade": false,
     "grade_id": "cell-4757be1a3ec18b56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# close window\n",
    "if enable_visualization:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80e6794175185b6f2cb14211cf015539",
     "grade": false,
     "grade_id": "cell-51bdd5ed623b2f16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "According to the code above, the code in the provided .py file, and the documentation of the environment, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e15be0abf28f4401bbfdde0f34298f1",
     "grade": false,
     "grade_id": "cell-8f1ad36de733ed92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 4.6.5: (2 Points)** What is the state for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac466974323fd6ffbfeaedb9f2e5992c",
     "grade": true,
     "grade_id": "cell-0a780f1afdcd6b1a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59b5ae2b898ecc2f1729e54dcff82c6a",
     "grade": false,
     "grade_id": "cell-50a080269bf6f296",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Task 4.6.6: (2 Points)** How often is the offline network updated to match the online one? Why do we need to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5dba2847e8894bd0aa6d6fddbe84cf22",
     "grade": true,
     "grade_id": "cell-099530ded38d7038",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a92986ddc86c07a5b6da88c2b490dcb9",
     "grade": false,
     "grade_id": "cell-db1ad2492dd6680a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Optional Questions**\n",
    "\n",
    "There may be three reasons for the episode to end:\n",
    "\n",
    "1. The cart slides too far away\n",
    "1. The pendulum falls too low\n",
    "1. 200 time steps have passed\n",
    "\n",
    "As mentioned before, we replace the Q target with the immediate reward only in case 1 and 2. In the third case however, the Q target remains untouched.\n",
    "\n",
    "Please answer the following questions:\n",
    "\n",
    "**Task 4.6.7: (3 Points)** How we treat this matter will have an influence on the Q-values being learned, and how they may be interpreted. Assuming we treat it as explained, and that we have managed to converge successfully, describe (in words and/or with mathematical expressions) what the Q-values we have learnt represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a75b9f4d363ad22196d9ef5c5d293f73",
     "grade": true,
     "grade_id": "cell-ef8fcf55948da326",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd15928af55f95207e275299652976aa",
     "grade": false,
     "grade_id": "cell-e000d9ccec2502d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Task 4.6.8: (2 Points)** If we would treat case 3 the same as case 1 and 2, we would actually end up with a Partially Observable Markov Decision Process (POMDP). What would we need to add to our observations, in order to obtain an observable MDP again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "164934878c5297e7cb11a6609baa9e8c",
     "grade": true,
     "grade_id": "cell-c6e9a35637f73eab",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a6271028b977453e6a9c87ee6f23130",
     "grade": false,
     "grade_id": "cell-0836fc1b783d1158",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Run the cell below to visualize your final policy (the greedy rather than epsilon-greedy one) in an episode from this environment.\n",
    "\n",
    "**Note:** In order to visualize, the env.render() command needs to work out on your system (see comment a few cells above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a866ba5ef5b225668433e92142bcd2ba",
     "grade": false,
     "grade_id": "cell-1e8a9b49909882ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "num_episodes = 10\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "if enable_visualization:\n",
    "    for i in range(num_episodes):\n",
    "            state = env.reset() #reset to initial state\n",
    "            state = state[None,:]\n",
    "            terminal = False # reset terminal flag\n",
    "            while not terminal:\n",
    "                #env.render()\n",
    "                time.sleep(.05)\n",
    "                with torch.no_grad():\n",
    "                    q_values = dqn.online_model(torch.tensor(state, dtype=torch.float, device=device)).cpu().numpy()\n",
    "                policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "                action = np.random.choice(num_actions, p=policy)\n",
    "                state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "                state = state[None,:]\n",
    "    # close window\n",
    "    env.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91c95cdee8f1715e789b8bdf2a4e2ff8",
     "grade": false,
     "grade_id": "cell-0bb5d237ca6839d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Plot the episodic rewards obtained throughout the optimization, together with a moving average of it (since the episodic reward is usually very noisy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a49cda53e12c1b8a976338c0f8bff7b9",
     "grade": false,
     "grade_id": "cell-a3c72b1dbffd2db4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=.4, label='R')\n",
    "avg_rewards = plt.plot(R_avg,label='avg R')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0, 210)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdaf694d1e01b5c18776df5e94d68585",
     "grade": false,
     "grade_id": "cell-293ec5dfa636ff48",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Congratulations, you have now successfully implemented the DQN algorithm. You are encouraged to explore different problems. There are a lot of different environments ready for you to implement your algorithms in. A few of these resources are:\n",
    "* [OpenAI gym](https://github.com/openai/gym)\n",
    "* [OpenAI Universe](https://github.com/openai/universe)\n",
    "* [DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)\n",
    "\n",
    "The model you implemented in this lab can be extended to solve harder problems. A good starting-point is to try to solve the Acrobot-problem, by loading the environment as \n",
    "\n",
    "**gym.make(\"Acrobot-v1\")**.\n",
    "\n",
    "The problem might require some modifications to how you decay $\\epsilon$, but otherwise, the code you have written within this lab should be sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fa7cf42dc4551c8b11e102771fef627",
     "grade": false,
     "grade_id": "cell-671cfb5a590863e9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.7 Atari games\n",
    "\n",
    "**Optional Question**\n",
    "\n",
    "A common benchmark for reinforcement learning algorithms is the old Atari games. Each timestep for the Atari games, the agent observes a screenshot as its current state.\n",
    "\n",
    "**Task 4.7.1: (3 Points)** There is an issue with the above definition of the agent state, what? Name at least two solutions to the problem, and why it wouldn't work well without these changes. \n",
    "\n",
    "Hint:\n",
    "- Imagine the game of pong. What is important for the algorithm to predict? What is the state of the agent? Is it possible to play the game optimally with this state formulation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7754cb75a59d2ada3ec98c4213d976e",
     "grade": true,
     "grade_id": "cell-55e109dd6169612b",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "64a2d671f44a448ea762e6157440981d": {
      "model_module": "ipysheet",
      "model_module_version": "~0.5.0",
      "model_name": "SheetModel",
      "state": {
       "column_headers": [
        "S0",
        "S1",
        "S2",
        "S3",
        "S4",
        "S5",
        "S6",
        "S7",
        "S8",
        "S9",
        "S10",
        "S11",
        "S12"
       ],
       "columns": 12,
       "layout": "IPY_MODEL_9efaf03a8183498bbdcda0e3f0a8fca0",
       "row_headers": [
        "S6",
        "S9"
       ],
       "rows": 2
      }
     },
     "9efaf03a8183498bbdcda0e3f0a8fca0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "height": "auto",
       "width": "auto"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
