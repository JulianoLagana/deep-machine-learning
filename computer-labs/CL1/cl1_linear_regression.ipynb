{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer lab 1 (CL1) - Linear regression\n",
    "\n",
    "The *python-crash-course* taught you general basics of python.\n",
    "The computer labs *CL1* and *CL2* will now specifically introduce you to python for machine learning.\n",
    "The goal is to provide you with a few hands-on examples of simple regression and classification so that you can build intuition for these types of problems and to show you a standard way of developing a machine learning algorithm in python.\n",
    "\n",
    "The labs will give you experience with a few well-known python libraries (called *modules*) which will be used in the course.\n",
    "Extra important is the machine learning module called Pytorch, but also libraries for data manipulation and visualisation, such as Pandas, Matplotlib and Numpy.\n",
    "\n",
    "We strongly encourage you to play with the code and explore ways of manipulating and plotting the data, that is how you build up your intuition.\n",
    "\n",
    "For this computer lab, we'll be using the IRIS dataset. Initially, we'll only look at a subset of it, and perform linear regression on two features of a given class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.  Import the necessary modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off using three different modules. The Numpy and Matplotlib modules were introduced in the `python-crash-course` and below we'll give a brief introduction to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# This last line is a special jupyter command\n",
    "# which makes matplotlib plots show up in notebooks themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Read the dataset from a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [IRIS dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) using Pandas. The method `read_csv(<filename>)` takes a path to a csv-file and returns a `DataFrame` object containing the data found in the file.\n",
    "\n",
    "Pandas data frames are a great way of handling simple data, where the entire dataset can be read into the computer memory all at once.\n",
    "Later, we will handle more complex data which will require us to create so called *Dataloaders*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file `iris.csv` is located in our current directory (<repo_root>/computer-labs/CL1),\n",
    "# otherwise the function would fail.\n",
    "dataset = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Analyze the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is comprised of morphologic data from three different species of the Iris flowers: Setosa, Virginica and Versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th> <center>Iris Setosa</center> </th>\n",
    "    <th> <center>Iris Virginica</center> </th> \n",
    "    <th> <center>Iris Versicolor</center> </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg\" alt=\"Iris Setosa\"></td>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg\" alt=\"Iris Virginica\"></td>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/Blue_Flag%2C_Ottawa.jpg\" alt=\"Iris Virginica\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lenght and width of both the petals and the sepals of each flower, together with its corresponding species were measured and stored in this dataset. Sepals and petals are both parts of a flower. Sepals are the outermost part of the whorl and the petals are the innermost part.\n",
    "![](http://terpconnect.umd.edu/~petersd/666/html/iris_with_labels.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what's inside the dataset now. The attribute `shape` of `DataFrame` objects returns the dimensions of the data inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, this dataset has 150 rows and 5 columns. It's easy to infer that this means 150 flowers were collected, and 5 different features were registered for each one. We can also take a closer look at them, using the method `head()`, which returns the first 5 rows by default (you can also pass a parameter to it, which specifies a different amount of rows to be shown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the header names for each column, together with the first rows, confirming that the species and morphologic measurements for each flower were collected. We can extract individual columns of this `DataFrame` by indexing using their names, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sepal_length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can check which species are present in the dataset using the `unique` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"species\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we see that only these three species are present in this dataset, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also learn more about the data types of each column with the method `info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the first four columns' elements are floating point numbers, and the last column's elements are objects (in this case, strings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Extract the desired data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this initial task, we are only interested in the setosa species. This corresponds to all the rows which have the column 'species' equal to the string 'setosa'. In order to extract these rows, we use [logical indexing in Pandas](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a boolean series, which for every row in the dataframe checks\n",
    "# whether a row is a setosa row or not.\n",
    "# We can use it to index our DataFrame object\n",
    "extract_rule = (dataset['species']=='setosa')\n",
    "print(extract_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the boolean series to index the DataFrame object.\n",
    "# This will give us a new dataframe which only contains the setosa rows\n",
    "setosa_dataset = dataset[extract_rule]\n",
    "setosa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract columns from the `DataFrame`.\n",
    "Suppose that we want to investigate the relationship between two features of this species, the 'sepal_length' and 'sepal_width'. To extract these, we [index the `DataFrame` using the name of the columns](https://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label)  we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = setosa_dataset['sepal_length'].values\n",
    "y = setosa_dataset['sepal_width'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the attribute `values` in a `DataFrame` object returns a numpy array.\n",
    "That is, extracting a single column in a `DataFrame` gives us a numpy array, not another `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The shape of a Numpy array (or Pytorch tensor)\n",
    "Let's take a short detour to stress the importance of the shape of an `ndarray`. This applies also to the Pytorch tensors we will use later.\n",
    "\n",
    "The shape is an attribute of the array and is represented by a tuple, e.g.:\n",
    "\n",
    "```python\n",
    "x.shape # outputs the tuple (50,)\n",
    "```\n",
    "Numpy makes a distinction between the shapes `(50,)` and `(50,1)`.\n",
    "To us this means the same thing, it's an object with 50 elements,\n",
    "but they are not interchangeable in Numpy and can produce unexpected results.\n",
    "In addition, some aggregating Numpy operations (like `mean` and `sum`) change the dimensions of the output.\n",
    "\n",
    "To prevent this and get consistent results to force the array into a certain shape.\n",
    "There are many ways to do this but it is good practice to use the `.reshape(<new_shape>)` function since it a) clearly communicates what we want the new shape to be and b) if we are mistaken about a shape, the code will fail explicitly and we can detect the bug at the source.\n",
    "\n",
    "Reshape the `x` and `y` arrays to reflect that they represent `num_samples` of 1 dimensional data.\n",
    "(We could probably get by without this particular reshape, but it's good to make it a habit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = x.shape[0]\n",
    "x = x.reshape((num_samples, 1))\n",
    "y = y.reshape((num_samples, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use matplotlib to plot all the examples in a 2D plane, where each dimension is one of the features described earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)\n",
    "ax.set_xlabel('sepal length')\n",
    "ax.set_ylabel('sepal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the relation between these features could be approximated using a linear function, such as \n",
    "$f(x) = w\\cdot x + b$. Let's try finding the parameters $w$ and $b$ that would make the best approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5  Guess the values of w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with some educated guesses. To make this more convenient, we'll first define a function to plot a scatter plot of the provided data, together with a straight line with parameters specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the data and a parameterized line\n",
    "def plot_data_and_line(w, b, x, y, ax, line_color='r', line_label=''):\n",
    "    \n",
    "    # Create points lying on the line\n",
    "    xline = np.unique(x)\n",
    "    yline = w*xline + b\n",
    "\n",
    "    # Plot both the line and the points from the dataset\n",
    "    ax.scatter(x,y, color='C0')\n",
    "    ax.plot(xline, yline, color=line_color, label=line_label)\n",
    "    ax.set_xlabel('sepal length')\n",
    "    ax.set_ylabel('sepal width') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data_and_line(1, -1, x, y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, another way of evaluating the quality of our approximation is to compute the MSE ([mean squared error](https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/)) between the true y features in the dataset and our predictions. So that we can use this value as well to guide our guesses, create a function to compute it (first, it might be beneficial to write down the analytical expression for it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84a068549164d7d4c62f18c3201a545b",
     "grade": true,
     "grade_id": "cell-17bd84c29b5dc802",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a function called `mse` to compute the mean squared error\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try different values of $w$ and $b$ and see how the resulting linear approximation looks like, compared to the scatter plot of our data. Using both the plot and the MSE, try searching for values of $w$ and $b$ that yield a good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc0d1c7d175ebd319359d29a0a453261",
     "grade": true,
     "grade_id": "cell-5df4a2b30a9d05e5",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Here, we just check that you actually implemented the `mse` function. \n",
    "try:\n",
    "    mse\n",
    "except NameError:\n",
    "    raise NotImplementedError(\n",
    "        \"You need to implement a function `mse` in the cell above. Don't forget to run the cell!\")\n",
    "\n",
    "# Guess the values for w and b\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Plot your guess\n",
    "fig, ax = plt.subplots()\n",
    "plot_data_and_line(w, b, x, y, ax)\n",
    "\n",
    "# Compute guess\n",
    "y_guess = w*x+b\n",
    "\n",
    "# Not strictly necessary but good practice to ensure same shape for predictions and target\n",
    "y_guess = y_guess.reshape(y.shape)\n",
    "\n",
    "# Compute and print the MSE of the guess\n",
    "print(\"MSE of your guess:\", mse(y,y_guess))\n",
    "\n",
    "# Sanity check: the MSE for the guess (w, b) = (1, -1) should be around 0.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Training a model with `autograd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the part where we start using Pytorch to actually train a model.\n",
    "Pytorch is a widely used library or *module* for doing machine learning in python.\n",
    "It allows the user to construct, train and evaluate general neural networks.\n",
    "\n",
    "It is not necessary for completing these labs, but if you are interested, take a look at Pytorch's\n",
    "- [documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [tutorials](https://pytorch.org/tutorials/)\n",
    "\n",
    "\n",
    "Pytorch contains a number of modules and classes that enable us to define and train neural networks in a compact and elegant manner. However, since it is so compact it may at first be challenging to understand what's going on under the hood. In order to clarify what the different modules do, we will first present a code example using basic Python commands and the `autograd` package from Pytorch. After that we will introduce standard Pytorch commands for expressing the code more compactly.\n",
    "\n",
    "More details on the `autograd` package can be found [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) and [here](https://pytorch.org/docs/stable/notes/autograd.html). Note that Pytorch works with tensors (type = `torch.Tensor`) but since they work very similarly to numpy arrays we do not teach them separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the low-level example we will use Pytorch to train a linear regressor that predict sepal width from sepal length.\n",
    "In order for `autograd` to work, we first need to change our data from Numpy arrays to Torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# A `numpy array` can easily be made into a `torch.Tensor´\n",
    "torch_x = torch.tensor(x, dtype=torch.float32)\n",
    "torch_y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if we don't provide the argument `dtype`, the `tensor` function tries to infer the type of the `Tensor` that will be created from the type of the data supplied. In this case, this is a problem, since `x_train` is of type `float64`:\n",
    "\n",
    "In deep learning, memory is usually a bottleneck, so it's common to use `float32` for the parameters of the neural network. Having the input (in this case `x_train`) as `float64`, and the parameters of the net as `float32` causes Pytorch to throw an error, and is a common error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Introducing the model parameters\n",
    "We will use the same simple linear (or rather affine) model $f(x) = w \\cdot x + b$.\n",
    "By turning on `requires_grad`, `autograd` will keep track of all operations performed on these tensors and help us compute gradients.\n",
    "\n",
    "We initialise the model parameters ($w, b$) by drawing values from a standard normal distribution, and show two ways of enabling the `requires_grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(1)\n",
    "w.requires_grad_()\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "# w and b are now singleton tensors:\n",
    "print(w)\n",
    "# We can access the actual value like this:\n",
    "print(w.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining the model and the loss function\n",
    "\n",
    "The model is an affine mapping from $x$ to $y$.\n",
    "We will train the model with the MSE loss.\n",
    "Pytorch and Numpy have very similar syntax so chances are that the MSE function you defined above would work as a loss function.\n",
    "The important thing is that it handles `Tensor`s as input so that `autograd` works.\n",
    "\n",
    "To make sure that the loss function works, we redefine the MSE function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: w, b as args\n",
    "def model(x, w, b):\n",
    "    \"\"\"Mean squared error loss\n",
    "    B = batch size\n",
    "    \n",
    "    Args:\n",
    "        x (Tensor): network input with shape (B, 1).\n",
    "    Returns:\n",
    "        mse_loss (float)\n",
    "    \"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    \"\"\"Mean squared error loss\n",
    "    B = batch size\n",
    "    \n",
    "    Args:\n",
    "        pred (Tensor): network output with shape (B, 1).\n",
    "        target (Tensor): network output with shape (B, 1).\n",
    "    Returns:\n",
    "        mse_loss (float)\n",
    "    \"\"\"\n",
    "    return ((pred - target)**2).mean()\n",
    "\n",
    "loss_fn = mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training the network\n",
    "To train the network we will do the following:\n",
    "\n",
    "* Select a minibatch of data.\n",
    "* Compute the loss for this data.\n",
    "* Perform a step of gradient descent.\n",
    "\n",
    "We do this for a number of epochs. To perform gradient descent, the method `.backward()` from the `autograd` packages is used to compute the gradients with respect to all variables for which `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs, batch size, number of training data and learning rate\n",
    "epochs = 20 \n",
    "batchsize = 32\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # We use this to compute the average loss in each epoch\n",
    "    losses = []\n",
    "    for i in range((num_samples - 1) // batchsize + 1):\n",
    "        #  Extracting the data in the current minibatch\n",
    "        start = i * batchsize\n",
    "        end   = start + batchsize\n",
    "        b_x   = torch_x[start:end]\n",
    "        b_y   = torch_y[start:end]\n",
    "        \n",
    "        # Computing the loss for the current minibatch\n",
    "        pred = model(b_x, w, b)\n",
    "        loss = loss_fn(pred,b_y)\n",
    "        losses.append(loss.item()) # and storing to print the number below\n",
    "        \n",
    "        # Performing a step of gradient descent\n",
    "        loss.backward()\n",
    "        # `autograd` registers every change to our parameters.\n",
    "        # However, the actual gradient step should not affect the gradient.\n",
    "        # Python has the key word `with` which creates a scope in our code.\n",
    "        # Here, we use it to temporarily disable `autograd`. After the indented block,\n",
    "        # `autograd` automatically resumes.\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad * lr\n",
    "            b -= b.grad * lr\n",
    "            # We need to end with weights.grad.zero_() and bias.grad.zero_()\n",
    "            # in order to reset the gradient calculations in autograd\n",
    "            # before computing the gradients for the next minibatch.\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Computing and printing the average loss in the current epoch\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    print('Epoch: {}\\tAvg loss: {}'.format(epoch, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our model has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data_and_line(w.item(), b.item(), torch_x, torch_y, ax)\n",
    "\n",
    "y_guess = w*torch_x+b\n",
    "y_guess = y_guess.reshape(torch_y.shape)\n",
    "print(\"MSE of the learned model:\", mse(torch_y,y_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Simplifying the code with Pytorch\n",
    "\n",
    "We will redo the simple example, but now using the full capabilities of the Pytorch module.\n",
    "\n",
    "There are a number of things that we can do to make the code shorter. Once you understand what the different modules do, shorter code also means that it is faster for you to read and write it, and it may also run faster. It will probably also be less prone to errors since we leverage on built-in commands to a much greater extent. We here consider three types of adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data loaders\n",
    "Pytorch expects to get data from a so called *data loader*.\n",
    "A data loader is an object that provides your machine learning model with *batches* of data.\n",
    "That way, we don't have to load our entire dataset into memory at once. Instead we get smaller, more manageable batches of data to work with.\n",
    "To create a `DataLoader` object, we first create a `TensorDataset`. The actual dataset can be stored in memory, on the hard drive or even on some remote server. `TensorDataset` is an abstraction over our data which provides methods for how to obtain a sample from our data.\n",
    "Finally, the `DataLoader` class takes an instance of our `TensorDataset` class (and some other configuration parameters) and automatically provides methods for iterating over the full data in batches.\n",
    "\n",
    "It is a bit overkill to do this with our simple Iris data (we have already seen how to do it with a `DataFrame`),\n",
    "but you will soon run into more complex data which do require a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current shape of `x` (and `y`) is (50,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x: {}\".format(x.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to be careful with the actual dimension of our dataset.\n",
    "\n",
    "The loader expects data of size `(N,D)`, where `N` is the number of samples in the dataset, and `D` is the dimension of each sample.\n",
    "We have already reshaped data to the correct dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset and the data loader.\n",
    "\n",
    "Note that in the data loader we specify the batch size to match the size of the dataset.\n",
    "Typically, we will use batch sizes which are much smaller than the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch_x, torch_y)\n",
    "data_loader = DataLoader(dataset, batch_size=len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model and loss function\n",
    "Second, we typically define our models as a child of the `nn.Module` class. This enables us to initiate and update all model parameters in a more unified fashion and we do not explicitly specify and update all the different weights and biases in the model.\n",
    "\n",
    "We define a class `LinearRegressor` which inherits from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"The __init__ function creates our model\n",
    "        by creating a network which only has a single linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Every Pytorch model has a forward method.\n",
    "        It describes how a network, or even a single layer transforms an input `x` to an output\n",
    "        \n",
    "        Here, we simple refer the input to our linear layer.\n",
    "        \"\"\"\n",
    "        return self.lin(x)\n",
    "\n",
    "# Create an instance of the model class.\n",
    "model = LinearRegressor()\n",
    "\n",
    "# `model` is a simple linear transformation. By default its parameters (w and b) are intialised randomly,\n",
    "# but we can still see how it works.\n",
    "\n",
    "# Let's create some made-up input data.\n",
    "# We need to jump through some hoops to make sure that the input is\n",
    "# - of type torch.Tensor\n",
    "# - with shape (1, 1)\n",
    "test_x = torch.tensor(2.4).reshape((1, 1))\n",
    "\n",
    "# Now we can use our model to predict y.\n",
    "# Note: model(x) is just a shortcut for model.forward(x).\n",
    "test_y = model(test_x)\n",
    "\n",
    "# y will change with every run, since the model parameters are randomly chosen.\n",
    "print(\"model({:.2f}) = {:.2f}\".format(test_x.item(), test_y.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still use the mean squared error loss.\n",
    "Although we have already implemented one above, we use the one built in to Pytorch.\n",
    "When possible, use the functions provided by the Pytorch module, it reduces potential bugs and is likely to be the most efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Optimisation\n",
    "Third, we can use the Pytorch package `torch.optim` to perform the gradient steps (and more advanced optimizations) in a convenient manner. To get this up and running, we need to import the package and define an optimiser.\n",
    "\n",
    "For optimisation, we use stochastic gradient descent (since our batch size is the size of the dataset, we're actually doing gradient descent).\n",
    "The optimizer will then perform the optimisation on the model parameters (conveniently accessible by `model.parameters()`), using the gradients computed using the `.backward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4  Putting it together and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the data loaders, the model, the loss function and the optimiser as above, we can train the network by simply following these steps in a loop:\n",
    "1. Sample a batch of data from our dataset\n",
    "2. Compute the model's prediction on the batch\n",
    "3. Compute the loss of the prediction w.r.t. ground-truth\n",
    "4. Backpropagate the loss through the model's parameters\n",
    "5. Perform one step of gradient descent.\n",
    "\n",
    "We will do this for 20 epochs. Again, since our batch size is the same size of the dataset, this means we'll take 20 steps of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    losses_in_epoch = []\n",
    "    for batch in data_loader:\n",
    "\n",
    "        # 1. These are the sampled batches of inputs and ground-truth\n",
    "        batch_x, batch_y = batch\n",
    "        # 2. Compute the model's prediction on the batch\n",
    "        pred = model(batch_x)\n",
    "        \n",
    "        # 3. Compute the loss of the prediction w.r.t. ground-truth\n",
    "        loss = loss_fn(pred, batch_y)\n",
    "        \n",
    "        # Save losses in a list for averaging later (not sctrictly necessary for batch_size = len(x))\n",
    "        losses_in_epoch.append(loss)\n",
    "        \n",
    "        # 4. Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. One step of gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the gradients computed in the backpropagation, for starting new optimization step\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print('Epoch: {}\\tLoss: {}'.format(epoch, sum(losses_in_epoch)/len(losses_in_epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the final MSE obtained (~0.06). Compare it to the one obtained in the `autograd` example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the networks performance we need to extract the parameters found by the optimisation by using the `parameters` method of the created model (this returns a [generator](https://www.programiz.com/python-programming/generator), so we transform it into a list first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in this list is a `Parameter` object, which is like a special case of a `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the underlying tensor by using the `data` attribute of the `Parameter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the float inside the tensor with the `item` method (only works with one-element tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[0].data.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together we can get the parameters of our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star, b_star = [p.data.item() for p in parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"w*: {:.3f}\".format(w_star))\n",
    "print(\"b*: {:.3f}\".format(b_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data_and_line(w_star, b_star, x, y, ax)\n",
    "y_guess = w*torch_x+b\n",
    "y_guess = y_guess.reshape(torch_y.shape)\n",
    "print(\"MSE of the learned model:\", mse(torch_y,y_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the problem considered above, this code is only moderately more compact (compared to the low-level version) but it will remain simple as we proceed to consider more complex problems. Remember, you can always learn more about Pytorch at [https://pytorch.org/](https://pytorch.org/) where you can find [tutorials](https://pytorch.org/tutorials/) and detailed [documentation](https://pytorch.org/docs/stable/index.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
