{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A general procedure for supervised learning\n",
    "\n",
    "**Note: This is part 2 of CL1, you should complete part 1: \"cl1_linear_regression.ipynb\" before continuing with this one.**\n",
    "\n",
    "In this second part we repeat the basics behind training a Pytorch model. This time we will do a classification problem. Classification with Pytorch is conceptually very similar to regression but there are some special features to mind.\n",
    "\n",
    "The goal is to make you comfortable with using Pytorch and also to show how such machine learning problems are generally solved.\n",
    "For most supervised learning tasks, the procedure we follow is comprised of the following steps:\n",
    "\n",
    "### Step 1: Data exploration\n",
    "The first step is normally to load the data and try to understand its properties. A few things that are usually useful:\n",
    "1. Check data formats.\n",
    "2. Visual inspection of data.\n",
    "3. Investigate (get some type of understanding for) how hard the problem is. \n",
    "\n",
    "\n",
    "### Step 2: Data preprocessing\n",
    "1. Normalise (or scale) input data. \n",
    "2. Convert the data to a different type, or organize it differently for the optimization (e.g. Numpy arrays, subsets of the dataset, etc.)\n",
    "3. Encode input and output data on a suitable form. For instance, we often use one-hot encoding to represent string variables.\n",
    "4. Split data into training, validation and test sets.\n",
    "\n",
    "\n",
    "### Step 3: Training\n",
    "1. Build a tentative network architecture (could be the simplest one you think could work, or based on previous sucesses). Here we'll do it in two ways to show you Pytorch behind the scenes.\n",
    "2. Select optimiser, performance measures and a few more hyperparameters. \n",
    "3. Train the network. \n",
    "4. Analyze performance on the training and validation sets. Adjust design decisions accordingly.\n",
    "\n",
    "\n",
    "### Step 4: Assessment\n",
    "1. Use the network for predictions in the test set.\n",
    "2. Evaluate the final quality of the model. **Note**: Once this is done, you shouldn't alter your model anymore, otherwise you need a new test set (if you want an honest estimate of your model's generalisation capacity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply most of these steps to the task of correctly classifying an Iris plant, given its morphologic features present in the IRIS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn is another visualisation module, much like matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we'll use all of the data, not only focusing on one of the species or a subset of the features. The `plot` method can help us obtain different types of visualizations of the data in the `DataFrame`. For instance, we can use it to plot histograms of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(kind='hist', bins=30, alpha=0.7, figsize=[15,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somewhat informative, but we could get an even better grasp of the data by first separating it into the different species (it seems likely that different species will have different feature distributions), and then plotting the histograms.\n",
    "\n",
    "However, if we let the `plot` method automatically create the histogram bins where it wants, each histogram might have different ranges, which would make it harder to compare them. Instead, we create the bins ourselves and pass that as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'species' column, so we get only the numeric values of the dataset\n",
    "features_dataset = dataset.drop('species', axis=1)\n",
    "\n",
    "# Find maximum and minimum values\n",
    "maxval = np.max(features_dataset.values)\n",
    "minval = np.min(features_dataset.values)\n",
    "\n",
    "# Create 30 linearly spaced numbers in this range\n",
    "my_bins = np.linspace(minval, maxval, 30)\n",
    "print(my_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the species\n",
    "species_names = dataset['species'].unique()\n",
    "print(species_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each species name, plot a histogram\n",
    "for name in species_names:\n",
    "    dataset[dataset[\"species\"]==name].plot(kind=\"hist\", bins=my_bins, alpha=0.7, figsize=[15,4], title=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that different species do have substantial differences in the distributions of each feature, e.g. the Setosa species has shorter sepals than the others, etc. \n",
    "\n",
    "Another way to gain more insight about the data is using the method `pairplot`, from the seaborn python module. This shows scatter plots between all feature pairs (hence the time required to run it increases exponentially with the number of features!) and histograms for each feature, color-coded by the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also helpful to check if the dataset is balanced. We can do so like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in a dictionary with the number of ocurrences of each species\n",
    "n = {}\n",
    "for name in species_names:\n",
    "    extract_rule = dataset['species']==name\n",
    "    n[name] = len(dataset[extract_rule])\n",
    "    \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that each species occurs exactly 50 times in the dataset, so it's perfectly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing input and output vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to prepare the data for the training. The first thing we should do is define the input and the output arrays for our network. \n",
    "\n",
    "Defining the input is as simple as extracting only the numeric columns of the dataset (this can also be conveniently done using the `drop` method, as done before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numerical values\n",
    "# `.values` extracts the data as an numpy array\n",
    "x = dataset[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "\n",
    "# Print first 10 rows\n",
    "print(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the output vector requires one more step, because of the way we'll train our network. Since the optimiser needs to be able to compare the predictions made by the neural network (i.e. a numeric vector), with the desired output vector in order to decide how to alter the weights, it's usually necessary to encode the output vector in a numeric format, instead of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['species'].values[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a handy function to map the species strings to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_species(species):\n",
    "    if species == 'setosa':\n",
    "        return 0\n",
    "    if species == 'versicolor':\n",
    "        return 1\n",
    "    if species == 'virginica':\n",
    "        return 2\n",
    "    else:\n",
    "        raise ValueError('Species \\'{}\\' is not recognized.'.format(species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use `map` to apply it to every element in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = map(encode_species, dataset['species'].values)\n",
    "y = np.array(list(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, in order to assess how well our classifier generalizes to new, unseen data, we would like to withhold part of the dataset from the training process. This withheld part is usually called the test set. \n",
    "\n",
    "Scikit-learn conveniently provides `train_test_split` function for exactly this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method randomly chooses which examples will be withheld, and here we want the test set to be comprised of approximately 30% of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `x_train` and `y_train` to train the network, and `x_test` and `y_test` to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# A `numpy array` can easily be made into a `torch.Tensor´\n",
    "torch_x = torch.tensor(x_train, dtype=torch.float32)\n",
    "torch_y = torch.tensor(y_train, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we choose from `float32` to reduce memory use for `x_train`,\n",
    "while for `y_train` the data are integers, representing class indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training\n",
    "For training we need to set up a data loader, model, loss function and optimiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data loaders\n",
    "First, we often create data loaders using `TensorDataset`and `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset = TensorDataset(torch_x, torch_y)\n",
    "t_data_loader = DataLoader(t_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these are in place, we can simply write \n",
    "```python\n",
    "for b_x, b_y in t_data_loader: \n",
    "```\n",
    "\n",
    "instead of \n",
    "\n",
    "```python\n",
    "for i in range((m - 1) // batchsize + 1):\n",
    "    #  Extracting the data in the current minibatch\n",
    "    start = i * batchsize\n",
    "    end   = start + batchsize\n",
    "    b_x   = torch_x[start:end]\n",
    "    b_y   = torch_y[start:end]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model, loss functions and optimiser\n",
    "We will use a neural network which only has one hidden layer. The input to this hidden layer are the features for each sample, which are 4-dimensional. The output of it is 3-dimensional, consisting of one number for each of the three classes, but what are these numbers?\n",
    "\n",
    "First let's us formulate what we actually want our model to do.\n",
    "For classification problems the most common represntation is a model that takes an input $\\x$ and predicts a probability vector $\\p$, where the element $\\p_c$ is the predicted probability of class $c$.\n",
    "This is typically done by transforming the output of the last layer $\\z$ with a softmax function:\n",
    "$$\n",
    "    \\p_c = \\frac{e^{\\z_c}}{\\sum_{c'=1}^{C} e^{\\z_{c'}}}\n",
    "$$\n",
    "(convince yourself that the softmax computes a proper probability vector for any input $\\z$).\n",
    "\n",
    "We think of $\\z$ as the actual output of the network because the softmax is just a deterministic transformation and not a learnable layer. This output $\\z$ is commonly referred to as *logits*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Our model inherits from `nn.Module`\n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LogisticRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, `model` becomes an instantiation of the class `LogisticRegression`. Instances of these classes are callable and `model(x)` directly evaluates the method `model.forward(x)`.\n",
    "\n",
    "Note that we define the model as being just the linear layer, the softmax function is instead included in the loss function (this is merely a design choice in our code).\n",
    "The particular loss function we use is Pytorch's [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), which applies the (log of the) softmax function for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearing up Pytorch's NLL/Cross entropy confusion\n",
    "$\n",
    "\\def\\x{\\mathbf{x}}\n",
    "\\def\\yTrue{\\mathbf{y}}\n",
    "\\def\\z{\\mathbf{z}}\n",
    "\\def\\p{\\mathbf{p}}\n",
    "\\def\\loss{\\mathcal{L}}\n",
    "$\n",
    "This is a slight digression but this is a common source for confusion in Pytorch so let's tackle it head on.\n",
    "When implementing a classifier model in Pytorch it is common to see two different loss functions:\n",
    "- [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- [`NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
    "\n",
    "Both are valid options for classification but they have slightly different behaviour so you need to know how to use them.\n",
    "\n",
    "The quantity we really want to use for our loss function is the cross entropy. It is defined as\n",
    "$$\n",
    "\\loss_{CE}(\\p, \\yTrue) =\n",
    "- \\sum_{c=1}^{C} \\yTrue_c \\log \\left(\n",
    "  \\p_c.\n",
    "\\right)\n",
    "$$\n",
    "Here, we represent the ground truth $\\yTrue$ (the true class) as a one-hot vector (a vector with all zeroes, except at the true class where it is one).\n",
    "Suppose that the true class is $c^*$, then the only non-zero element of $\\yTrue$ is $\\yTrue_{c^*} = 1$,\n",
    "meaning that the loss above can be simplified:\n",
    "$$\n",
    "\\loss_{CE}(\\p, \\yTrue) =\n",
    "- \\log \\left(\n",
    "    \\p_{c^*}\n",
    "\\right).\n",
    "$$\n",
    "Examining the above loss, we see that this is exactly the negative log likelihood loss $\\loss_{NLL}$\n",
    "\n",
    "With this problem formulation (probability and one-hot vectors) the cross entropy loss and NLL loss are equivalent.\n",
    "If they are equivalent, what is the problem?\n",
    "Well, for some reason the `CrossEntropy` in Pytorch is defined to take the logits $\\z$ as input, not the probability vector $\\p$.\n",
    "Instead the `CrossEntropy` loss applies log softmax to the input itself, before actually calculating the loss we defined above.\n",
    "Recall our current network `LogisticRegressor` above (let's call it $f$ to save some writing).\n",
    "It does not apply softmax at the end of the `forward` method. Instead it outputs the logits, i.e. $f(\\x) = \\z$. This is to comply with the `CrossEntropy` loss.\n",
    "\n",
    "In contrast, the `NLLLoss` is implemented to take a probability vector as input, *not* logits.\n",
    "If we instead modify our network into a new network $\\tilde{f}$, which ends with a softmax function,\n",
    "then the network itself would output a class probability vector $\\p$ and we would use the `NLLLoss`.\n",
    "\n",
    "Both options calculate the same loss in the end but we have to carefully choose the correct combination of network architecture and loss.\n",
    "**Make sure your network outputs what your loss function expects!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, back to the training procedure\n",
    "\n",
    "#### The gradient descent steps\n",
    "\n",
    "Again, we use the Pytorch package `torch.optim` to perform the gradient steps.\n",
    "In this case, we have set the optimiser to the popular Adam algorithm, which is usually significantly better than the mini-batch gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Putting it together and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the data loaders, the model, the loss function and the optimizer as above, we can train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    losses = []\n",
    "    for b_x, b_y in t_data_loader:\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    \n",
    "    print('Epoch: {}\\tAvg loss: {}'.format(epoch, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Improving the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we trained the model above we clearly saw that the loss decreases as the optimization progresses. However, it's hard to know if a certain value of the loss, say 0.5, is good or bad, so we're not sure about the performance of the model. It's usually informative to display other metrics of progress during training.\n",
    "\n",
    "One such metric that is easily interpretable is the accuracy of the model, defined as:\n",
    "\n",
    "$ Acc = \\frac{\\# \\text{Samples correctly classified}}{\\# \\text{Samples}} $\n",
    "\n",
    "From the definition, we see that accuracy is always a value between 0 and 1. An accuracy of 0 means that every single prediction made by our model is wrong, and an accuracy of 1 that our model is always correct.\n",
    "\n",
    "**Task**: what if our dataset was imbalanced? Would it still be a good idea to use accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know if a certain sample was predicted correctly by the model, we'll use the class with the highest predicted probability as the choice to compare with the ground truth. For instance, these are the input features and ground truth for the first sample in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x, sample_y = t_dataset[0]\n",
    "print(sample_x)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the prediction according to our trained model.\n",
    "Remember that our network does not have a softmax function at the end.\n",
    "Instead the network outputs the logits. If we were to end with a softmax transformation,\n",
    "the class index with the highest probability would of course be the index with the largest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits predicted by the model\n",
    "model(sample_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the index of the highest prediction as the \"hard\" prediction of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sample_x).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the prediction with the ground truth, to know if the model was correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sample_x).argmax() == sample_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this for all samples during training for computing the accuracy. Additionally, the accuracy for each batch is not as informative as the accuracy in the entire dataset, so we will aggregate the number of correct predictions and display it once for each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model and the optimizer\n",
    "model = LogisticRegressor()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    acc_ = 0.0\n",
    "    for b_x, b_y in t_data_loader:\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Compute number of correct predictions\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct += (pred.argmax(dim=1) == b_y).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    accuracy = n_correct/len(t_dataset)\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    \n",
    "    print('Epoch: {}\\tAvg loss: {:.3f} \\tAccuracy: {:.2f}'.format(epoch, avg_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's easier to interpret the model's performance (you should get a result close to 100% classification).\n",
    "\n",
    "**Task:** Were you expecting overfitting? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Adding validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't train the model for enough steps, it won't reach a satisfactory performance. If we train it for too many steps, it might overfit to the training data. To balance this tradeoff (and to tune hyper-parameters in general) training data is split into two sets.  One is used to actually perform the backpropagation and update the weights (usually to as the *training set*), and the other which is only used, not for backpropagation, but to assess the model's performance (usually referred to as the *validation set*). This way we can train the model with the training set and use the performance on the validation set to determine if we are overfitting.\n",
    "\n",
    "We can easily split our existing `t_dataset` using the `random_split` function from Pytorch. This function accepts as first argument the dataset we want to split and as the second argument a sequence of lengths for the new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_t_dataset, val_t_dataset = random_split(t_dataset, [90, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create data loaders for each of the sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_data_loader = DataLoader(train_t_dataset, batch_size=32, shuffle=True)\n",
    "val_t_data_loader = DataLoader(val_t_dataset, batch_size=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we perform the optimization, compute the train and validation loss as well as accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the model and the optimizer\n",
    "model = LogisticRegressor()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    # Compute predictions and back-prop in the training set\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    for b_x, b_y in train_t_data_loader:\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct += (pred.argmax(dim=1) == b_y).sum().item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    train_accuracy = n_correct/len(train_t_dataset)\n",
    "    train_avg_loss = sum(losses)/len(losses)    \n",
    "\n",
    "        \n",
    "    # Compute predictions in the validation set (with adagrad deactivated)\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_t_data_loader:\n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += (pred.argmax(dim=1) == b_y).sum().item()\n",
    "        val_accuracy = n_correct/len(val_t_dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)      \n",
    "        \n",
    "        \n",
    "    display_str = 'Epoch {}'\n",
    "    display_str += '\\tLoss: {:.3f} '\n",
    "    display_str += '\\tLoss (val): {:.3f}'\n",
    "    display_str += '\\tAccuracy: {:.2f}'\n",
    "    display_str += '\\tAccuracy (val): {:.2f}'\n",
    "    print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clearly see if the model is performing well and whether or not it's overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we would like to be able to evaluate how well the model can predict the class of new, unseen samples. This was the reason for withholding part of our data from the training process, so that now we have fresh, unseen samples. \n",
    "\n",
    "The idea now is to use the trained model to predict the class of each new sample, given its features, and then compare the predicted label with the correct label for each sample.\n",
    "\n",
    "---\n",
    "\n",
    "To compare the labels, we can use different techniques. As we saw before, we can compute the accuracy, but this time on the test set samples. However, although this helps us to evaluate the model's performance, it provides an incomplete picture. For instance, it doesn't explain the types of missclassifications we are doing.\n",
    "\n",
    "So that we can gather more information about the quality of our classifier, we'll also compute the confusion matrix of its predictions. The confusion matrix is a table layout of the predictions of the classifier, in which each row represents the labels of the predicted class and each column the labels of the correct class.\n",
    "\n",
    "---\n",
    "\n",
    "To illustrate, imagine we train a classifier on samples that are either from the 'dog' class or the 'cat' class. After training, we show it 50 new samples. 30 of these new samples are cats, and 20 are dogs.\n",
    "\n",
    "For the new cats, our classifier correctly predicts 28 of them, but in 2 samples it thinks they are from the 'dog' class. Further, the classifier correctly predicts 15 of the new dogs, and in 5 samples it thinks they are actually from the 'cat' class. \n",
    "\n",
    "The resulting confusion matrix for this example would be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\" rowspan=\"2\"></th>\n",
    "      <th colspan=\"2\"><b>Predicted label</b></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Cat</td>\n",
    "    <td>Dog</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td rowspan=\"2\"><b>True label</b></td>\n",
    "    <td>Cat</td>\n",
    "    <td>28</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Dog</td>\n",
    "    <td>5</td>\n",
    "    <td>15</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the element $C_{ij}$ ($i$-th row, $j$-th column), corresponds to the number of predictions of class $i$, when the true known class was the $j$-th class. This is not universal: some sources define the confusion matrix as the transpose of the one shown here. However, `sklearn` defines confusion matrices like this, so we'll adhere to this definition.\n",
    "\n",
    "A handy way of computing the confusion matrix, given the predictions and the true labels, is to use the function [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) from the scikit-learn module.\n",
    "\n",
    "The first step is to compute our predictions in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor(x_test, dtype=torch.float32)\n",
    "test_labels = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "preds = model(test_samples).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to compute the accuracy, we'll do like we did during training, but for the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (preds == test_labels).sum().item()/len(preds)\n",
    "print(\"Accuracy: %.2f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can compute the confusion matrix using the `confusion_matrix` method from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(test_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: What can you conclude from this confusion matrix? Which classes are easy/hard to separate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
